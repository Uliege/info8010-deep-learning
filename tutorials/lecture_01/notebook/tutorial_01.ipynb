{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors!\n",
    "\n",
    "Tensors are one of the main ingredients when it comes to modern Deep-Learning frameworks. Almost all deep learning computations can be expressed as tensor operations which make computation fast and efficient. In this practical we will see how to manipulate tensors within the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the type of the tensor\n",
    "t.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the tensor\n",
    "\n",
    "print(t.size())\n",
    "print(t.numel())\n",
    "print(t.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the tensor\n",
    "\n",
    "r = torch.Tensor(t)\n",
    "r.resize_(3, 8)\n",
    "print(r)\n",
    "print(r.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors (1D Tensors) and similarities with Numpy\n",
    "\n",
    "Some of you might have noticed some similarities between the way PyTorch deals with tensors and the way Numpy deals with arrays. When it comes to basic algebraic operations both libraries can indeed be very similar as shown in some of the examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "np_v = np.array([1, 2, 3, 4]).astype(int)\n",
    "\n",
    "print(v)\n",
    "print(np_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a second tensor with the same properties\n",
    "\n",
    "w = torch.Tensor([1, 0, 2, 0])\n",
    "np_w = np.array([1, 0, 2, 0]).astype(int)\n",
    "\n",
    "print(w)\n",
    "print(np_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise multiplication\n",
    "\n",
    "x = v * w\n",
    "np_x = np.multiply(np_v, np_w)\n",
    "\n",
    "print(x)\n",
    "print(np_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing: extract sub-Tensor [from:to)\n",
    "\n",
    "print(x[0:3])\n",
    "print(np_x[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with integers ranging from 1 to 5, excluding 5\n",
    "\n",
    "v = torch.arange(1, 4 + 1)\n",
    "np_v = np.arange(1, 5)\n",
    "\n",
    "print(v)\n",
    "print(np_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square all elements in the tensor\n",
    "\n",
    "print(v.pow(2))\n",
    "print(np.square(np_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add one dimension: matrices (2D Tensors)\n",
    "\n",
    "Within deep learning, the fun really starts when increasing the dimensionality of the tensors.\n",
    "In fact there is not so much we can do when dealing with one-dimensional tensors. Just like before, 2D tensors are treated in a similar way between PyTorch and Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x4 tensor\n",
    "\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])\n",
    "print(m)\n",
    "\n",
    "np_m = np.array([[2, 5, 3, 7],[4, 2, 1, 9]])\n",
    "print(np_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the tensor\n",
    "\n",
    "print(m.size())\n",
    "print(np_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "\n",
    "m[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing column 1, all rows (returns size 2)\n",
    "\n",
    "m[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Tensor Operations\n",
    "\n",
    "It is now up to you to get familiar with some basic tensor operations in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing column 1, all rows (returns size 2x2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes row 0, all columns (returns 1x4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a random tensor of size 2x4 to m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract a random tensor of size 2x4 to m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply a random tensor of size 2x4 to m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide m by a random tensor of size 2x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose tensor m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Polynomial Regression\n",
    "\n",
    "In this morning's lecture we have seen Polynomial Regression and how simple it is to solve such an optimization problem. In this exercise I will first show you how this simple model can be trained and it will then be up to you to code the solution in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def true_f(x):\n",
    "    return 0.1 * (x-2) ** 3 + x ** 2 - 8.0 * x - 1.0\n",
    "\n",
    "def generate(n_samples):\n",
    "    X = np.random.rand(n_samples) * 20.0 - 10.0\n",
    "    y = true_f(X) + 5 * np.random.randn(n_samples)\n",
    "    \n",
    "    return X.reshape(n_samples, 1), y\n",
    "\n",
    "X_train, y_train = generate(15)\n",
    "xs = np.linspace(-10, 10, num=1000)\n",
    "plt.plot(xs, true_f(xs), c=\"r\", label=\"$g(x)$\")\n",
    "plt.scatter(X_train, y_train, label=\"$y = g(x) + \\epsilon$\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(color=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, beta, poly):\n",
    "    Xp = poly.transform(X)\n",
    "    return np.dot(Xp, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=True)\n",
    "Xp = poly.fit_transform(X_train)\n",
    "beta = np.dot(np.dot(np.linalg.inv(np.dot(Xp.T, Xp)), Xp.T), y_train) # analytic solution for the optimization problem\n",
    "\n",
    "error = np.mean((y_train - model(X_train, beta, poly)) ** 2) # cost-function\n",
    "\n",
    "plt.plot(xs, true_f(xs), c=\"r\", label=\"$g(x)$\")\n",
    "plt.scatter(X_train, y_train, label=\"$y = g(x) + \\epsilon$\")\n",
    "plt.plot(xs, model(xs.reshape(-1,1), beta, poly), c=\"b\", label=\"$\\hat{y} = f(x)$\")\n",
    "plt.title(\"degree = %d, $\\hat{R}(f, d) = %.2f$\" % (3, error))\n",
    "plt.ylim(-40, 80)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"The error of our model is {}\".format(error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lottery",
   "language": "python",
   "name": "lottery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
